{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e590b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize     #Tokenizes text into words.\n",
    "from nltk.corpus import stopwords            #Provides a list of common stopwords in various languages.\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer     #Used for stemming and lemmatization, \n",
    "from nltk import pos_tag             #Assigns parts of speech to words in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac181494",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is the subfield of linguistic, computer science and artificial intelligence concerned with the interaction between human and computer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5406ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is the subfield of linguistic computer science and artificial intelligence concerned with the interaction between human and computer\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "filtered_text = re.sub(r'[^\\w\\s]','',text)     #removes punctuation marks from the text\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3b8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'the', 'subfield', 'of', 'linguistic', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interaction', 'between', 'human', 'and', 'computer']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(filtered_text)         #splits the text into individual words or tokens.\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ee7b83-d1fe-493f-bb30-42218bca84a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bhargavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd1b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'subfield', 'linguistic', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'human', 'computer']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))                       #removingstopwords\n",
    "filtered_tokens = [x for x in tokens if x.lower() not in stop_words]\n",
    "print(filtered_tokens)\n",
    "#Stopwords are common words that do not carry significant meaning in the context of text analysis. They are removed to focus on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e469250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('subfield', 'VBD'), ('linguistic', 'JJ'), ('computer', 'NN'), ('science', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('interaction', 'NN'), ('human', 'JJ'), ('computer', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged = pos_tag(filtered_tokens)             #Assigns a part-of-speech tag to each word in the text.\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa80e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'human', 'comput']\n"
     ]
    }
   ],
   "source": [
    "p = PorterStemmer()                         # reduces words to their root or base form.\n",
    "stem = [p.stem(x) for x in filtered_tokens]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138fc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'subfield', 'linguistic', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'human', 'computer']\n"
     ]
    }
   ],
   "source": [
    "l = WordNetLemmatizer()                       # reduces words to their base form but considers the context and meaning of the word.\n",
    "lemma = [l.lemmatize(x) for x in filtered_tokens]\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07e36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(x):\n",
    "    if x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    elif x.startswith('J'):\n",
    "        return 'a'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b52aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'subfield', 'linguistic', 'computer', 'science', 'artificial', 'intelligence', 'concern', 'interaction', 'human', 'computer']\n"
     ]
    }
   ],
   "source": [
    "l = WordNetLemmatizer()                                #maps NLTK POS tags to WordNet POS tags for more accurate lemmatization\n",
    "lemma = [l.lemmatize(x,pos(w)) for x,w in pos_tagged]\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52610a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer          #Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef1464cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" \".join(lemma)\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "tfidf_matrix = tv.fit_transform([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71e83a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial --->  0.2581988897471611\n",
      "computer --->  0.5163977794943222\n",
      "concern --->  0.2581988897471611\n",
      "human --->  0.2581988897471611\n",
      "intelligence --->  0.2581988897471611\n",
      "interaction --->  0.2581988897471611\n",
      "language --->  0.2581988897471611\n",
      "linguistic --->  0.2581988897471611\n",
      "natural --->  0.2581988897471611\n",
      "processing --->  0.2581988897471611\n",
      "science --->  0.2581988897471611\n",
      "subfield --->  0.2581988897471611\n"
     ]
    }
   ],
   "source": [
    "features = tv.get_feature_names_out()                 #TF-IDF score for each word in the document.\n",
    "\n",
    "tfidf_score = dict(zip(features, tfidf_matrix.toarray()[0]))\n",
    "\n",
    "for word, score in tfidf_score.items():\n",
    "    print(f\"{word} --->  {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "241d561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial --->  0.2581988897471611\n",
      "computer --->  0.5163977794943222\n",
      "concern --->  0.2581988897471611\n",
      "human --->  0.2581988897471611\n",
      "intelligence --->  0.2581988897471611\n",
      "interaction --->  0.2581988897471611\n",
      "language --->  0.2581988897471611\n",
      "linguistic --->  0.2581988897471611\n",
      "natural --->  0.2581988897471611\n",
      "processing --->  0.2581988897471611\n",
      "science --->  0.2581988897471611\n",
      "subfield --->  0.2581988897471611\n"
     ]
    }
   ],
   "source": [
    "data = \" \".join(lemma)\n",
    "\n",
    "tv = TfidfVectorizer(use_idf=True)\n",
    "tfidf_matrix = tv.fit_transform([data])\n",
    "features = tv.get_feature_names_out()                  ## or tv.get_feature_names()\n",
    "\n",
    "tfidf_score = dict(zip(features, tfidf_matrix.toarray()[0]))\n",
    "\n",
    "for word, score in tfidf_score.items():\n",
    "    print(f\"{word} --->  {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "357fcfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial --->  0.2581988897471611\n",
      "computer --->  0.5163977794943222\n",
      "concern --->  0.2581988897471611\n",
      "human --->  0.2581988897471611\n",
      "intelligence --->  0.2581988897471611\n",
      "interaction --->  0.2581988897471611\n",
      "language --->  0.2581988897471611\n",
      "linguistic --->  0.2581988897471611\n",
      "natural --->  0.2581988897471611\n",
      "processing --->  0.2581988897471611\n",
      "science --->  0.2581988897471611\n",
      "subfield --->  0.2581988897471611\n"
     ]
    }
   ],
   "source": [
    "data = \" \".join(lemma)\n",
    "\n",
    "tv = TfidfVectorizer(use_idf=False)\n",
    "tfidf_matrix = tv.fit_transform([data])\n",
    "features = tv.get_feature_names_out()\n",
    "\n",
    "tfidf_score = dict(zip(features, tfidf_matrix.toarray()[0]))\n",
    "\n",
    "for word, score in tfidf_score.items():\n",
    "    print(f\"{word} --->  {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daed889-05f1-4347-b915-1b3d1866a3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a13684-dda8-4471-b59a-cb3b289e7df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43225252-ff49-4e48-892a-ad25b99ac50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8c0c2-a2d7-4257-979f-02506fe317c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03400b-61b9-4af1-9e50-62732113d6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab928c1d-2d0f-4165-b495-2e9657093dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bdd11-2b44-40a9-aa66-b37e3e9b6a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568cec6-13cf-4573-9f5d-8f657f7b96ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470b8dc-d223-4ce3-b1fc-022944a9a9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca0642-8f88-46ad-9e83-1dba102cdd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d1369-45b1-4062-b74c-03989120172d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45511e25-63f3-4a34-87a8-0801b9421008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98f20bf-bd02-439c-85f1-8807a78b5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # this code preprocesses text by removing punctuation and stopwords, tokenizes it, performs stemming, lemmatization, and finally computes the TF-IDF scores for each word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35bd0d8c-87fb-4818-99f6-0557559c792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is the process of breaking down text into smaller units, typically words or tokens.\n",
    "# Stemming is the process of reducing words to their root or base form (stem), often by removing suffixes or prefixes.\n",
    "# Lemmatization is similar to stemming but more sophisticated. It involves reducing words to their base or dictionary form (lemma) based on their meaning and context.\n",
    "# For example, the word \"better\" would be lemmatized to \"good\", and \"running\" would be lemmatized to \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039cb9c-ea33-4eb3-984b-eab03143fec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
